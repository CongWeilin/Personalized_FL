{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from options import args_parser\n",
    "from update import LocalUpdate, test_inference\n",
    "from models import MLP, CNNMnist, CNNFashion_Mnist, CNNCifar\n",
    "from utils import get_dataset, average_weights, exp_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.argv=['']; del sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(dataset='mnist', epochs=10, frac=1.0, gpu=0, iid=1, kernel_num=9, kernel_sizes='3,4,5', local_bs=50, local_ep=10, lr=0.01, max_pool='True', model='mlp', momentum=0.5, norm='batch_norm', num_channels=1, num_classes=10, num_filters=32, num_users=100, optimizer='sgd', seed=1, stopping_rounds=10, unequal=0, verbose=1)\n",
      "\n",
      "Experimental details:\n",
      "    Model     : mlp\n",
      "    Optimizer : sgd\n",
      "    Learning  : 0.01\n",
      "    Global Rounds   : 10\n",
      "\n",
      "    Federated parameters:\n",
      "    IID\n",
      "    Fraction of users  : 1.0\n",
      "    Local Batch size   : 50\n",
      "    Local Epochs       : 10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# guarantee reproducible results\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(0)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# define paths\n",
    "path_project = os.path.abspath('..')\n",
    "\n",
    "args = args_parser()\n",
    "\n",
    "print(args)\n",
    "exp_details(args)\n",
    "\n",
    "if args.gpu:\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "device = 'cuda' if args.gpu else 'cpu'\n",
    "\n",
    "# load dataset and user groups\n",
    "train_dataset, test_dataset, user_groups = get_dataset(args)\n",
    "\n",
    "# BUILD MODEL\n",
    "if args.model == 'cnn':\n",
    "    # Convolutional neural netork\n",
    "    if args.dataset == 'mnist':\n",
    "        global_model = CNNMnist(args=args)\n",
    "    elif args.dataset == 'fmnist':\n",
    "        global_model = CNNFashion_Mnist(args=args)\n",
    "    elif args.dataset == 'cifar':\n",
    "        global_model = CNNCifar(args=args)\n",
    "\n",
    "elif args.model == 'mlp':\n",
    "    # Multi-layer preceptron\n",
    "    img_size = train_dataset[0][0].shape\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "        global_model = MLP(dim_in=len_in, dim_hidden=64,\n",
    "                           dim_out=args.num_classes)\n",
    "else:\n",
    "    exit('Error: unrecognized model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (layer_input): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_hidden): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Set the model to train and send it to device.\n",
    "global_model.to(device)\n",
    "global_model.train()\n",
    "print(global_model)\n",
    "\n",
    "# copy weights\n",
    "global_weights = global_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplit(Dataset):\n",
    "    \"\"\"An abstract Dataset class wrapped around Pytorch Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, idxs):\n",
    "        self.dataset = dataset\n",
    "        self.idxs = [int(i) for i in idxs]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idxs)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image, label = self.dataset[self.idxs[item]]\n",
    "        return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "def train_val_test(dataset, idxs):\n",
    "    \"\"\"\n",
    "    Returns train, validation and test dataloaders for a given dataset\n",
    "    and user indexes.\n",
    "    \"\"\"\n",
    "    # split indexes for train, validation, and test (80, 10, 10)\n",
    "    idxs_train = idxs[:int(0.8*len(idxs))]\n",
    "    idxs_val = idxs[int(0.8*len(idxs)):int(0.9*len(idxs))]\n",
    "    idxs_test = idxs[int(0.9*len(idxs)):]\n",
    "    # print(len(idxs_train), len(idxs_val), len(idxs_test))\n",
    "\n",
    "    trainloader = DataLoader(DatasetSplit(dataset, idxs_train),\n",
    "                             batch_size=args.local_bs, shuffle=True)\n",
    "    validloader = DataLoader(DatasetSplit(dataset, idxs_val),\n",
    "                             batch_size=int(len(idxs_val)/10), shuffle=False)\n",
    "    testloader = DataLoader(DatasetSplit(dataset, idxs_test),\n",
    "                            batch_size=int(len(idxs_test)/10), shuffle=False)\n",
    "    return trainloader, validloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.optimizer == 'sgd':\n",
    "    optimizer_global = torch.optim.SGD(global_model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer_global = torch.optim.Adam(global_model.parameters(), lr=args.lr, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, dataloader):\n",
    "    \"\"\" Returns the inference accuracy and loss.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    total, correct = 0.0, 0.0\n",
    "    loss = list()\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Inference\n",
    "        outputs = model(images)\n",
    "        batch_loss = criterion(outputs, labels)\n",
    "        loss += [batch_loss.item()]\n",
    "\n",
    "        # Prediction\n",
    "        _, pred_labels = torch.max(outputs, 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        correct += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "\n",
    "    accuracy = correct/total\n",
    "    loss = sum(loss)/len(loss)\n",
    "    return accuracy, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activate users 100/100\n",
      "time 0: 3.907278\n",
      "global 0, acc 0.082500, loss 2.354677\n",
      "time 1: 3.194634\n",
      "global 1, acc 0.093000, loss 2.337307\n",
      "time 2: 2.675622\n",
      "global 2, acc 0.106333, loss 2.320618\n",
      "time 3: 2.476659\n",
      "global 3, acc 0.118500, loss 2.304654\n",
      "time 4: 2.448809\n",
      "global 4, acc 0.130167, loss 2.289295\n",
      "time 5: 2.984103\n",
      "global 5, acc 0.141167, loss 2.274468\n",
      "time 6: 3.210317\n",
      "global 6, acc 0.151667, loss 2.260193\n",
      "time 7: 2.408530\n",
      "global 7, acc 0.163000, loss 2.246439\n",
      "time 8: 5.769344\n",
      "global 8, acc 0.175167, loss 2.233244\n",
      "time 9: 2.503040\n",
      "global 9, acc 0.185667, loss 2.220306\n",
      "time 10: 3.378320\n",
      "global 10, acc 0.195833, loss 2.207575\n",
      "time 11: 3.022441\n",
      "global 11, acc 0.206167, loss 2.195015\n",
      "time 12: 3.336100\n",
      "global 12, acc 0.218000, loss 2.182695\n",
      "time 13: 3.376979\n",
      "global 13, acc 0.228167, loss 2.170603\n",
      "time 14: 3.691071\n",
      "global 14, acc 0.241167, loss 2.158451\n",
      "time 15: 3.667515\n",
      "global 15, acc 0.256500, loss 2.146487\n",
      "time 16: 2.341017\n",
      "global 16, acc 0.274333, loss 2.134680\n",
      "time 17: 3.366382\n",
      "global 17, acc 0.293833, loss 2.122851\n",
      "time 18: 4.013530\n",
      "global 18, acc 0.315833, loss 2.110956\n",
      "time 19: 4.103654\n",
      "global 19, acc 0.336167, loss 2.099189\n"
     ]
    }
   ],
   "source": [
    "args.gamma = 0.5\n",
    "\n",
    "# prepare dataloaders for each client\n",
    "trainloader, validloader, testloader = dict(), dict(), dict()\n",
    "trainloader_iterator, validloader_iterator, testloader_iterator = dict(), dict(), dict()\n",
    "local_acc, local_loss = dict(), dict()\n",
    "\n",
    "for idx in range(args.num_users):\n",
    "    idxs = list(user_groups[idx])\n",
    "    trainloader[idx], validloader[idx], testloader[idx] = train_val_test(train_dataset, idxs)\n",
    "    trainloader_iterator[idx] = iter(trainloader[idx])\n",
    "    validloader_iterator[idx] = iter(validloader[idx])\n",
    "    testloader_iterator[idx]  = iter(testloader[idx])\n",
    "    local_acc[idx] = list()\n",
    "    local_loss[idx] = list()\n",
    "    \n",
    "global_acc = []\n",
    "global_loss = []\n",
    "\n",
    "num_users = max(int(args.frac * args.num_users), 1)\n",
    "print('activate users %d/%d'%(num_users, args.num_users))\n",
    "\n",
    "for global_iter in range(20):\n",
    "   \n",
    "    idxs_users = np.random.choice(range(args.num_users), num_users, replace=False)\n",
    "    \n",
    "    # save local grad gradient\n",
    "    local_grads = dict()\n",
    "    for name, params in global_model.named_parameters():\n",
    "        if params.requires_grad:\n",
    "            local_grads[name] = torch.zeros_like(params.data)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for idx in idxs_users:\n",
    "        # load single mini-batch\n",
    "        try:\n",
    "            images, labels = next(trainloader_iterator[idx])\n",
    "        except StopIteration:\n",
    "            trainloader_iterator[idx] = iter(trainloader[idx])\n",
    "            images, labels = next(trainloader_iterator[idx])\n",
    "\n",
    "        # create local model\n",
    "        model = copy.deepcopy(global_model)\n",
    "        if args.optimizer == 'sgd':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "        elif args.optimizer == 'adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "        # train local model\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        log_probs = model(images)\n",
    "        loss = criterion(log_probs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        for name, params in model.named_parameters():\n",
    "            if params.requires_grad:\n",
    "                if global_iter>0:\n",
    "                    params.grad.data = params.grad.data*args.gamma + snap_grads[name]*(1-args.gamma)\n",
    "                local_grads[name] += params.grad.data\n",
    "        optimizer.step() \n",
    "        \n",
    "        # test local model\n",
    "        acc, loss = inference(model, testloader[idx])\n",
    "        local_acc[idx] += [acc]\n",
    "        local_loss[idx] += [loss]\n",
    "        # print('local %d, acc %f, loss %f'%(idx, acc, loss))\n",
    "        \n",
    "    print('time %d: %f'%(global_iter, time.time()-start_time))\n",
    "    \n",
    "    # make sure global_model has grad.data\n",
    "    global_model.train()\n",
    "    if global_iter==0:\n",
    "        log_probs = global_model(images)\n",
    "        loss = criterion(log_probs, labels)\n",
    "        loss.backward()\n",
    "    \n",
    "    optimizer_global.zero_grad()\n",
    "    for name, params in global_model.named_parameters():\n",
    "        if params.requires_grad:\n",
    "            local_grads[name] = local_grads[name]/args.num_users\n",
    "            params.grad = local_grads[name]\n",
    "    snap_grads = copy.deepcopy(local_grads)\n",
    "    \n",
    "    # test global model\n",
    "    list_acc, list_loss = [], [] \n",
    "    for idx in idxs_users:\n",
    "        acc, loss = inference(global_model, testloader[idx])\n",
    "        list_acc.append(acc)\n",
    "        list_loss.append(loss)\n",
    "    list_acc = sum(list_acc)/len(list_acc)\n",
    "    list_loss = sum(list_loss)/len(list_loss)\n",
    "    global_acc += [list_acc]\n",
    "    global_loss += [list_loss]\n",
    "    \n",
    "    print('global %d, acc %f, loss %f'%(global_iter, list_acc, list_loss))\n",
    "    optimizer_global.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results_gamma_%.1f'%(args.gamma), 'wb') as f:\n",
    "    pickle.dump([global_acc, global_loss, local_acc, local_loss], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "##################################\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "gamma_list = [0.1, 0.5, 1]\n",
    "for gamma in gamma_list:\n",
    "    with open('results_gamma_%.1f'%(gamma), 'rb') as f:\n",
    "        [global_acc, global_loss, local_acc, local_loss] = pickle.load(f)\n",
    "    y = global_acc\n",
    "    x = np.arange(len(y))\n",
    "    axs.plot(x,y,label='gamma=%.1f'%(gamma))\n",
    "    \n",
    "plt.title('Glocal accuracy / Global communication')\n",
    "axs.set_xlabel('Global communication')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('global_acc.pdf')\n",
    "plt.close()\n",
    "\n",
    "##################################\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    with open('results_gamma_%.1f'%(gamma), 'rb') as f:\n",
    "        [global_acc, global_loss, local_acc, local_loss] = pickle.load(f)\n",
    "    y = global_loss\n",
    "    x = np.arange(len(y))\n",
    "    axs.plot(x,y,label='gamma=%.1f'%(gamma))\n",
    "    \n",
    "plt.title('Global loss / Global communication')\n",
    "axs.set_xlabel('Global communication')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('global_loss.pdf')\n",
    "plt.close()\n",
    "\n",
    "##################################\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    with open('results_gamma_%.1f'%(gamma), 'rb') as f:\n",
    "        [global_acc, global_loss, local_acc, local_loss] = pickle.load(f)\n",
    "        \n",
    "    acc = []\n",
    "    for idx in range(args.num_users):\n",
    "        acc.append(local_acc[idx])\n",
    "    acc = np.mean(acc,0)\n",
    "        \n",
    "    y = acc\n",
    "    x = np.arange(len(y))\n",
    "    axs.plot(x,y,label='gamma=%.1f'%(gamma))\n",
    "    \n",
    "plt.title('Average local accuracy / Global communication')\n",
    "axs.set_xlabel('Global communication')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('local_acc.pdf')\n",
    "plt.close()\n",
    "\n",
    "##################################\n",
    "fig, axs = plt.subplots()\n",
    "\n",
    "for gamma in gamma_list:\n",
    "    with open('results_gamma_%.1f'%(gamma), 'rb') as f:\n",
    "        [global_acc, global_loss, local_acc, local_loss] = pickle.load(f)\n",
    "        \n",
    "    loss = []\n",
    "    for idx in range(args.num_users):\n",
    "        loss.append(local_loss[idx])\n",
    "    loss = np.mean(loss,0)\n",
    "        \n",
    "    y = loss\n",
    "    x = np.arange(len(y))\n",
    "    axs.plot(x,y,label='gamma=%.1f'%(gamma))\n",
    "    \n",
    "plt.title('Average local loss / Global communication')\n",
    "axs.set_xlabel('Global communication')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.grid(True)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('local_loss.pdf')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09416667, 0.105     , 0.11516667, 0.1285    , 0.142     ,\n",
       "       0.1505    , 0.16483333, 0.172     , 0.184     , 0.19433333,\n",
       "       0.20083333, 0.21366667, 0.22666667, 0.24033333, 0.2565    ,\n",
       "       0.27216667, 0.29616667, 0.31433333, 0.33733333, 0.3535    ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = []\n",
    "for idx in range(args.num_users):\n",
    "    acc.append(local_acc[idx])\n",
    "np.mean(acc,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = np.mean(acc,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
